wordcloud(words = df_word$word, # 뿌려질 단어들
freq = df_word$freq,  # 단어 빈도
min.freq = 3,         # 최소 단어 빈도
max.words = 100,      # 나타날 최대 단어 수
random.order = F,     # 최빈 단어를 중앙에 배치
rot.per = 0.1,        # 회전 단어 비율
scale = c(2, 0.7),    # 단어 크기 범위
colors = topo.colors(16))         # 단어 색상
wordcloud(words = df_word$word, # 뿌려질 단어들
freq = df_word$freq,  # 단어 빈도
min.freq = 3,         # 최소 단어 빈도
max.words = 100,      # 나타날 최대 단어 수
random.order = F,     # 최빈 단어를 중앙에 배치
rot.per = 0.1,        # 회전 단어 비율
scale = c(2, 0.7),    # 단어 크기 범위
colors = pal)         # 단어 색상
colors = topo.colors(16)         # 단어 색상
wordcloud(words = df_word$word, # 뿌려질 단어들
freq = df_word$freq,  # 단어 빈도
min.freq = 3,         # 최소 단어 빈도
max.words = 100,      # 나타날 최대 단어 수
random.order = F,     # 최빈 단어를 중앙에 배치
rot.per = 0.1,        # 회전 단어 비율
scale = c(2, 0.7),    # 단어 크기 범위
colors = topo.colors(16))         # 단어 색상
# 2. 국정원 트윗 데이터 텍스트마이닝
rm(list=ls())
twitter <- read.csv('inData/twitter.csv',
header = TRUE,
stringsAsFactors = F,
fileEncoding = 'utf-8')
head(twitter)
class(twitter)
View(twitter)
twitter <- rename(twitter,
no = 번호, id = 계정이름, date = 작성일, tw = 내용용)
twitter <- rename(twitter,
no = 번호, id = 계정이름, date = 작성일, tw = 내용)
View(twitter)
# 2.2 불필요한 문자 삭제
twitter$tw <- str_replace_all(twitter$tw, '\\w', ' ')
twitter$tw <- str_replace_all(twitter$tw, '[ㄱ-ㅎ]', ' ')
View(twitter)
# 2.1 데이터 불러오기
twitter <- read.csv('inData/twitter.csv',
header = TRUE,
stringsAsFactors = F,
fileEncoding = 'utf-8')
head(twitter)
class(twitter)
View(twitter)
twitter$tw <- str_replace_all(twitter$tw, '[ㄱ-ㅎ]',' ')
# 2.2 불필요한 문자 삭제
twitter$tw <- str_replace_all(twitter$tw, '\\W',' ')
twitter$tw <- str_replace_all(twitter$tw, '[ㄱ-ㅎ]',' ')
View(twitter)
head(twitter)
# 2.3 명사 추출
nouns <- extractNoun(twitter$tw)
class(nouns)
# 2.4 워드 카운트
wordcount <- table(unlist(nouns))
class(wordcount)
df_word <- as.data.frame(wordcount, stringsAsFactors = F)
str(df_word)
df_word <- rename(df_word, word = Var1, freq = Freq)
df_word <- as.data.frame(wordcount, stringsAsFactors = F)
str(df_word)
str(df_word)
df_word
# 2.1 데이터 불러오기
twitter <- read.csv('inData/twitter.csv',
header = TRUE,
stringsAsFactors = F,
fileEncoding = 'utf-8')
twitter <- rename(twitter,
no = 번호, id = 계정이름, date = 작성일, tw = 내용)
# 2.2 불필요한 문자 삭제
twitter$tw <- str_replace_all(twitter$tw, '\\W',' ')
View(twitter)
# 2.3 명사 추출
nouns <- extractNoun(twitter$tw)
class(nouns)
class(wordcount)
# 2.4 워드 카운트
wordcount <- table(unlist(nouns))
class(wordcount)
wordcount
df_word <- as.data.frame(wordcount, stringsAsFactors = F)
str(df_word)
df_word
df_word <- rename(df_word, word = Var1, freq = Freq)
head(df_word)
str(df_word)
df_word <- filter(df_word, nchar(word) > 1)
head(df_word)
top20
# 최빈단어 top20 도표 & 시각화
top20 <- df_word[order(df_word$freq, decreasing = T), ][1:20, ]
top20
top20 <- df_word %>%
arrange(desc(freq)) %>%
head(20)
ggplot(top20, aes(x = freq, y = reorder(word, freq))) + # 시각화
geom_col() +
geom_text(aes(label = freq), hjust = 1, size = 3, col = 'yellow')
# 2.5 워드클라우드 그리기
pal <- brewer.pal(9, 'Blues')[5:9]
wordcloud(words = df_word$word,
freq = df_word$freq,
min.freq = 5,
max.words = 200,
random.order = F,
rot.per = 0.1,
scale = c(3,0.3),
colors = pal)
# 1. 정적 웹 크롤링; 단일 페이지 크롤링 (rvest패키지 사용)
install.packages("rvest")
library(rvest)
url <- 'https://movie.naver.com/movie/point/af/list.nhn'
text <- read_html(url, encoding = 'CP949')
text
# 영화제목; .movie
nodes <- html_nodes(text, '.movie')
title <- html_text(nodes)
# 해당 영화 페이지
movieInfo <- html_attr(nodes, 'href')
movieInfo <- paste0(url, movieInfo)
movieInfo
# 평점; .list_netizen_score em
nodes <- html_nodes(text, ".list_netizen_score em")
nodes
review <- html_text(nodes, trim = TRUE)
review
review <- gsub('\t', '', review)
review <- gsub('\n', '', review)
review
review <- unlist(strsplit(review, '중[0-9]{1,2}'))[seq(2,20,2)]
review <- gsub('신고', '', review)
review
page <- cbind(title, movieInfo)
page <- cbind(page, point)
# 평점; .list_netizen_score em
nodes <- html_nodes(text, ".list_netizen_score em")
nodes
point <- html_text(nodes)
point
# 영화 리뷰; .title
review <- html_text(nodes, trim = TRUE)
review
review <- gsub('\t', '', review)
review <- gsub('\n', '', review)
review
review <- unlist(strsplit(review, '중[0-9]{1,2}'))[seq(2,20,2)]
review
page <- cbind(page, point)
page <- cbind(title, movieInfo)
page <- cbind(page, review)
View(page)
url <- 'https://movie.naver.com/movie/point/af/list.nhn'
text <- read_html(url, encoding = 'CP949')
text
# 영화제목; .movie
nodes <- html_nodes(text, '.movie')
title <- html_text(nodes)
# 해당 영화 페이지
movieInfo <- html_attr(nodes, 'href')
movieInfo <- paste0(url, movieInfo)
movieInfo
# 평점; .list_netizen_score em
nodes <- html_nodes(text, ".list_netizen_score em")
nodes
point
point <- html_text(nodes)
point
# 영화 리뷰; .title
review <- html_text(nodes, trim = TRUE)
review
review <- gsub('\t', '', review)
review <- gsub('\n', '', review)
review
review <- unlist(strsplit(review, '중[0-9]{1,2}'))[seq(2,20,2)]
review <- gsub('신고', '', review)
review
page <- cbind(title, movieInfo)
page <- cbind(page, point)
page <- cbind(page, review)
View(page)
# csv 파일로 out
write.csv(page, 'outData/13_movie_review.csv', row.names = F)
# 1.2 여러 페이지 정적 웹 크롤링
home <- 'https://movie.naver.com/movie/point/af/list.nhn'
# 1.2 여러 페이지 정적 웹 크롤링
home <- 'https://movie.naver.com/movie/point/af/list.nhn'
site <- 'https://movie.naver.com/movie/point/af/list.nhn?&page='
movie.review <- NULL
text <- read_html(url, encoding = 'CP949')
for(i in 1:100) {
url <- paste0(site, i);
text <- read_html(url, encoding = 'CP949')
# 영화제목; .movie
nodes <- html_nodes(text, '.movie')
title <- html_text(nodes)
# 해당 영화 페이지
movieInfo <- html_attr(nodes, 'href')
movieInfo <- paste0(url, movieInfo)
# 평점; .list_netizen_score em
nodes <- html_nodes(text, ".list_netizen_score em")
point <- html_text(nodes)
# 영화 리뷰; .title
nodes <- html_nodes(text, '.title')
review <- html_text(nodes, trim = TRUE)
review <- gsub('\t', '', review)
review <- gsub('\n', '', review)
review <- unlist(strsplit(review, '중[0-9]{1,2}'))[seq(2,20,2)]
review <- gsub('신고', '', review)
page <- cbind(title, movieInfo)
page <- cbind(page, point)
page <- cbind(page, review)
movie.review <- rbind(movie.review, page)
}
View(movie.review)
# 영화 리뷰; .title
nodes <- html_nodes(text, '.title')
review <- html_text(nodes, trim = TRUE)
review
review <- gsub('\t', '', review)
review <- gsub('\n', '', review)
review
review <- unlist(strsplit(review, '중[0-9]{1,2}'))[seq(2,20,2)]
review <- gsub('신고', '', review)
review
page <- cbind(title, movieInfo)
page <- cbind(page, point)
page <- cbind(page, review)
View(page)
# csv 파일로 out
write.csv(page, 'outData/13_movie_review.csv', row.names = F)
write.csv(movie.review, 'outData/13_movie_review100pages.csv')
# csv 파일로 out
write.csv(page, 'outData/13_movie_review.csv')
write.csv(page, 'outData/13_movie_review1.csv', row.names = F)
# 위의 크롤링한 영화들 중 평점이 갖아 높은 10개를 시각화하기
library(dplyr)
library(ggplot2)
library(KoNLP)
library(wordcloud)
movie <- as.data.frame(movie.review, stringsAsFactors = F)
str(movie)
movie$point <- as.numeric(movie$point)
movie$point
result <- movie %>%
group_by(title) %>%
summarise(point.mean = mean(point),
point.sum = sum(point),
n = n()) %>%
arrange(desc(point.mean), desc(point.sum)) %>%
head(10)
result
ggplot(result,
aes(x = point.sum, y = reorder(title, point.sum)), vjust = 1) +
geom_col(aes(fill = title)) +
labs(title = "평점 top 10 영화", x = '총 평점') +
geom_text(aes(label = paste('총점: ', point.sum,
'평균: ', point.mean)), hjust = 0) +
coord_cartesian(xlim = c(0, 120)) +
ggplot(result,
aes(x = point.sum, y = reorder(title, point.sum)), vjust = 1) +
geom_col(aes(fill = title)) +
labs(title = "평점 top 10 영화", x = '총 평점') +
geom_text(aes(label = paste('총점: ', point.sum,
'평균: ', point.mean)), hjust = 0) +
coord_cartesian(xlim = c(0, 120)) +
theme(axis.title.y = element_blank(),
legend.position = "none")
ggplot(result,
aes(x = point.sum, y = reorder(title, point.sum)), vjust = 1) +
geom_col(aes(fill = title)) +
labs(title = "평점 top 10 영화", x = '총 평점') +
geom_text(aes(label=paste('총점:',point.sum,'평균:',point.mean)), hjust=0) +
coord_cartesian(xlim = c(0, 120)) +
theme(axis.title.y = element_blank(),
legend.position = "none")
# 리뷰의 최빈 단어 10개 워드클라우드 시각화
useNIADic()
movie$review <- gsub('\\W', ' ', movie$review)
movie$review <- gsub('[ㄱ-ㅎ]', ' ', movie$review)
View(movie)
nouns <- extractNoun(movie$review)
nouns
df_word <- as.data.frame(wordcount, stringsAsFactors = F)
df_word <- rename(df_word, word = Var1, freq = Freq)
View(df_word)
df_word <- filter(df_word, nchar(word)>1)
top20 <- df_word[order(df_word$freq, decreasing = T), ][1:20, ]
top20
df_word <- as.data.frame(wordcount, stringsAsFactors = F)
wordcount <- title(unlist(nouns))
df_word <- as.data.frame(wordcount, stringsAsFactors = F)
df_word <- rename(df_word, word = Var1, freq = Freq)
View(df_word)
df_word <- filter(df_word, nchar(word)>1)
df_word <- filter(df_word, nchar(word)>1)
top20 <- df_word[order(df_word$freq, decreasing = T), ][1:20, ]
# 리뷰의 최빈 단어 20개 워드클라우드 시각화
useNIADic()
movie$review <- gsub('\\W', ' ', movie$review)
movie$review <- gsub('[ㄱ-ㅎ]', ' ', movie$review)
View(movie)
nouns <- extractNoun(movie$review)
wordcount <- title(unlist(nouns))
wordcount <- table(unlist(nouns))
df_word <- as.data.frame(wordcount, stringsAsFactors = F)
df_word <- rename(df_word, word = Var1, freq = Freq)
View(df_word)
df_word <- filter(df_word, nchar(word)>1)
top20 <- df_word[order(df_word$freq, decreasing = T), ][1:20, ]
top20
ggplot(top20, aes(x = freq, y = reorder(word, freq))) +
geom_col() +
geom_text(aes(label = freq), hjust = 1, size = 3, col = 'yellow')
pal <- brewer.pal(8, 'Dark2')
wordcloud(words = df_word$word,
freq = df_word$freq,
min.freq = 5,
max.words = 200,
random.order = F,
rot.per = 0.1,
scale = c(4, 0.3),
colors = pal)
# 2. 동적 웹 크롤링(셀레니움 패키지 이용) : 스크롤다운, 로그인이후, 버튼,...
# 필요한 패키지 다운로드 & 로드
install.packages("RSelenium")
library(httr)
library(rvest)
library(RSelenium)
# 2.1 특정 부분에 텍스트 입력 후 엔터한 결과 크롤링하기
remDr <- remoteDriver(port = 4445L, # 포트번호
browserName = 'chrome') # 사용할 브라우저저
remDr$open
remDr$open()
# 2.1 특정 부분에 텍스트 입력 후 엔터한 결과 크롤링하기
remDr <- remoteDriver(port = 4445L, # 포트번호
browserName = 'chrome') # 사용할 브라우저
remDr$open()
library(httr)
library(rvest)
library(RSelenium)
# 2.1 특정 부분에 텍스트 입력 후 엔터한 결과 크롤링하기
remDr <- remoteDriver(port = 4445L, # 포트번호
browserName = 'chrome') # 사용할 브라우저
remDr$open()
# 필요한 패키지 다운로드 & 로드
install.packages("RSelenium")
# 2.1 특정 부분에 텍스트 입력 후 엔터한 결과 크롤링하기
remDr <- remoteDriver(port = 9515L, # 포트번호
browserName = 'chrome') # 사용할 브라우저
library(httr)
library(rvest)
library(RSelenium)
# 2.1 특정 부분에 텍스트 입력 후 엔터한 결과 크롤링하기
remDr <- remoteDriver(port = 9515L, # 포트번호
browserName = 'chrome') # 사용할 브라우저
remDr$open()
# 2.1 특정 부분에 텍스트 입력 후 엔터한 결과 크롤링하기
remDr <- remoteDriver(port = 4445L, # 포트번호
browserName = 'chrome') # 사용할 브라우저
remDr$open()
# 2.1 특정 부분에 텍스트 입력 후 엔터한 결과 크롤링하기
remDr <- remoteDriver(port = 4444L, # 포트번호
browserName = 'chrome') # 사용할 브라우저
remDr$open()
docker run -d -p 4445:4444 selenium/standalone-chrome:88.0
# 필요한 패키지 다운로드 & 로드
install.packages("RSelenium")
install.packages("RSelenium")
install.packages("RSelenium")
install.packages("RSelenium")
install.packages("RSelenium")
library(httr)
library(rvest)
library(RSelenium)
# 2.1 특정 부분에 텍스트 입력 후 엔터한 결과 크롤링하기
remDr <- remoteDriver(port = 4444L, # 포트번호
browserName = 'chrome') # 사용할 브라우저
# 2.1 특정 부분에 텍스트 입력 후 엔터한 결과 크롤링하기
remDr <- remoteDriver(port = 4445L, # 포트번호
browserName = 'chrome') # 사용할 브라우저
remDr$open()
# 2.1 특정 부분에 텍스트 입력 후 엔터한 결과 크롤링하기
system("sudo docker pull selenium/standalone-chrome")
remDr <- remoteDriver(port = 4445L, # 포트번호
browserName = 'chrome') # 사용할 브라우저
remDr$open()
# 2.1 특정 부분에 텍스트 입력 후 엔터한 결과 크롤링하기
system("sudo docker pull selenium/standalone-chrome", wait = T)
Sys.sleep(5)
system("sudo docker run -d -p 4445:4444 selenium/standalone-chrome",wait=T)
system("sudo docker run -d -p 4445:4444 selenium/standalone-chrome",wait=T)
Sys.sleep(5)
remDr <- remoteDriver(port = 4445L, # 포트번호
browserName = 'chrome') # 사용할 브라우저
Sys.sleep(15)
remDr$open()
library(httr)
library(rvest)
library(RSelenium)
# 2.1 특정 부분에 텍스트 입력 후 엔터한 결과 크롤링하기
system("sudo docker pull selenium/standalone-chrome", wait = T)
Sys.sleep(5)
system("sudo docker run -d -p 4445:4444 selenium/standalone-chrome",wait=T)
Sys.sleep(5)
Sys.sleep(15)
remDr$open()
remDr <- remoteDriver(port = 4445L, # 포트번호
browserName = 'chrome') # 사용할 브라우저
remDr$open()
remDr$open()
remDr <- remoteDriver(port = 4444L, # 포트번호
browserName = 'chrome') # 사용할 브라우저
remDr$open()
# 1. 미국 주별 강력 범죄율 시각화
View(USArrests)
# 1.1 패키지 설치 및 로드
install.packages("ggiraphExtra")
install.packages("mapproj")
install.packages("maps")
library(ggiraphExtra)
library(mapproj)
library(maps)
library(ggplot2)
library(tibble) # 행이름(지역명)을 변수로 하기 위해
# 1.2 행 이름을 변수로
head(USArrests, 1)
crime <- rownames_to_column(USArrests, var = "state")
head(crime, 3)
head(crime, 3)
crime$state <- tolower(crime$state) # 주 이름들을 소문자로
head(crime, 3)
# (3) 미국 지도 주 정보 가져오기
state_map <- map_data("state")
# (4) 지도 시각화
ggChoropleth(data = crime,         # 지도에 표현할 데이터
aes(fill = Assault,   # 지도에 채워질 변수
map_id = state),  # 지역 변수
map = state_map,      # 위도 경도 지도 데이터
interactive = T)      # 인터렉티브 속성성
# 2. 대한민국 시도별 인구, 결핵 환자수  단계 구분도 만들기
install.packages("stringi")
install.packages("stringi")
install.packages("devtools")
devtools::install_github("cardiomoon/kormaps2014")
library(kormaps2014)
head(korpop1)
library(dplyr)
# 2.2 변수명 변경
korpop1 <- rename(korpop1,
pop = 총인구_명,
name = 행정구역별_읍면동동)
# 2.2 변수명 변경
korpop1 <- rename(korpop1,
pop = 총인구_명,
name = 행정구역별_읍면동)
str(changeCode(korpop1))
head(korpop1)
head(changeCode(kormap1))
head(changeCode(korpop1[, c('name', 'pop', 'code')]))
korpop1$name <- iconv(korpop1$name, 'UTF-8', 'CP949')
# 2.4 지도 시각화
ggChoropleth(data = korpop1,
aes(fill = pop,
map_id = code,
tooltip = name),
map = kormap1,
interactive = T)
library(mapproj)
# 2.4 지도 시각화
ggChoropleth(data = korpop1,
aes(fill = pop,
map_id = code,
tooltip = name),
map = kormap1,
interactive = T)
# 2.4 지도 시각화
ggChoropleth(data = korpop1,
aes(fill = pop,
map_id = code,
tooltip = name),
map = kormap1,
interactive = T)
library(ggiraphExtra)
library(mapproj)
library(maps)
library(ggplot2)
library(tibble) # 행이름(지역명)을 변수로 하기 위해
# 2.4 지도 시각화
ggChoropleth(data = korpop1,
aes(fill = pop,
map_id = code,
tooltip = name),
map = kormap1,
interactive = T)
head(changeCode(tbc))
tbc$name1 <- iconv(tbc$name1, 'UTF-8', 'CP949')
tbc$name <- iconv(tbc$name, 'UTF-8', 'CP949')
head(tbc)
ggChoropleth(data = tbc,
aes(fill = NewPts,
map_id = code,
tooltip = name1),
map = kormap1,
interactive = T)
